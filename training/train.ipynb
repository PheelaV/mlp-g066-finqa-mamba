{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train LLMs, optionally LoRA or distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this notebook produces `train.py`, just execute `jupyter nbconvert --to script 'train.ipynb'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, TrainingArguments\n",
    "import datasets\n",
    "\n",
    "import utils\n",
    "import custom_training\n",
    "\n",
    "\n",
    "def is_interactive():\n",
    "    import __main__ as main\n",
    "\n",
    "    return not hasattr(main, \"__file__\")\n",
    "\n",
    "\n",
    "def load_config(json_filepath):\n",
    "    if os.path.isfile(json_filepath):\n",
    "        try:\n",
    "            with open(json_filepath, \"r\") as f:\n",
    "                return json.load(f)\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Error loading configuration file {json_filepath}: {e}. Using default command line arguments.\"\n",
    "            )\n",
    "    else:\n",
    "        print(\n",
    "            f\"Configuration file {json_filepath} not found. Using default command line arguments.\"\n",
    "        )\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(args, model_name):\n",
    "    \"\"\"\n",
    "        Load the tokenizer and set the special tokens, specific to the model\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    if \"mamba\" in args.base_model or \"pythia\" in args.base_model:\n",
    "        # these were defaults either way\n",
    "        tokenizer.eos_token = \"<|endoftext|>\"\n",
    "        tokenizer.pad_token = \"<|padding|>\"\n",
    "        tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)\n",
    "        tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "    return tokenizer\n",
    "\n",
    "def get_dataset(args, tokenizer):\n",
    "    \"\"\"\n",
    "        Load the dataset and apply tokenization\n",
    "    \"\"\"\n",
    "    dataset_list = utils.load_dataset(args.dataset, args.from_remote)\n",
    "    dataset_train = datasets.concatenate_datasets([d[\"train\"] for d in dataset_list]).shuffle(seed=42)\n",
    "    if args.test_dataset:\n",
    "        dataset_list = utils.load_dataset(args.test_dataset, args.from_remote)\n",
    "    dataset_test = datasets.concatenate_datasets([d[\"test\"] for d in dataset_list])\n",
    "    dataset = datasets.DatasetDict({\"train\": dataset_train, \"test\": dataset_test})\n",
    "    # Display first sample from the training dataset\n",
    "    print(dataset[\"train\"][0])\n",
    "    # Filter out samples that exceed the maximum token length and remove unused columns\n",
    "    dataset = dataset.map(partial(utils.tokenize, args, tokenizer, prompt_in_label=True))\n",
    "    print(\"original dataset length: \", len(dataset[\"train\"]))\n",
    "    dataset = dataset.filter(lambda x: not x[\"exceed_max_length\"])\n",
    "    print(\"filtered dataset length: \", len(dataset[\"train\"]))\n",
    "    dataset = dataset.remove_columns([\"instruction\", \"input\", \"output\", \"exceed_max_length\"])\n",
    "    return dataset\n",
    "\n",
    "def get_trainer(args, model, tokenizer, dataset, formatted_time):\n",
    "    \"\"\"\n",
    "        Create the trainer and training arguments\n",
    "    \"\"\"\n",
    "\n",
    "    common_args = {\n",
    "        \"output_dir\":f\"finetuned_models/{args.run_name}_{formatted_time}\",\n",
    "        \"logging_steps\":args.log_interval,\n",
    "        \"num_train_epochs\":args.num_epochs,\n",
    "        \"dataloader_num_workers\":args.num_workers,\n",
    "        \"learning_rate\":args.learning_rate,\n",
    "        \"warmup_ratio\":args.warmup_ratio,\n",
    "        \"lr_scheduler_type\":args.scheduler,\n",
    "        \"save_steps\":args.eval_steps,\n",
    "        \"eval_steps\":args.eval_steps,\n",
    "        \"evaluation_strategy\":args.evaluation_strategy,\n",
    "        \"load_best_model_at_end\":args.load_best_model,\n",
    "        \"remove_unused_columns\":False,\n",
    "        \"report_to\":\"wandb\",\n",
    "        \"run_name\":args.run_name,\n",
    "        \"fp16\": args.fp16 & torch.cuda.is_available(),\n",
    "        \"logging_dir\":\"./logs\"\n",
    "    }\n",
    "        \n",
    "    if args.distributed:\n",
    "        distributed_args = {\n",
    "            \"deepspeed\": args.ds_config,\n",
    "            \"per_device_train_batch_size\": args.batch_size,\n",
    "            \"per_device_eval_batch_size\": args.batch_size,\n",
    "            \"gradient_accumulation_steps\": args.gradient_steps,\n",
    "            \"deepspeed\": args.ds_config,\n",
    "        }\n",
    "        common_args.update(distributed_args)\n",
    "\n",
    "    training_args = TrainingArguments(**common_args)\n",
    "    \n",
    "    if args.lora_rank > 0:\n",
    "        from peft import (\n",
    "            LoraConfig,\n",
    "            # get_peft_model,\n",
    "            TaskType\n",
    "        )\n",
    "        if \"mamba\" in args.base_model:\n",
    "            peft_config =  LoraConfig(\n",
    "                    r=args.lora_rank,\n",
    "                    target_modules=utils.lora_module_dict[args.base_model],\n",
    "                    task_type=TaskType.CAUSAL_LM,\n",
    "                    bias=\"none\"\n",
    "                )\n",
    "        else:\n",
    "            peft_config = LoraConfig(\n",
    "                task_type=TaskType.CAUSAL_LM,\n",
    "                inference_mode=False,\n",
    "                r=args.lora_rank,\n",
    "                lora_alpha=32,\n",
    "                lora_dropout=0.1,\n",
    "                target_modules=utils.lora_module_dict[args.base_model],\n",
    "                bias=\"none\",\n",
    "            )\n",
    "        trainer = custom_training.CustomSFTTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            peft_config=peft_config,\n",
    "            train_dataset=dataset[\"train\"],\n",
    "            eval_dataset=dataset[\"test\"],\n",
    "            data_collator=custom_training.CustomDataCollatorSeq2Seq(tokenizer, padding=True),\n",
    "        )\n",
    "        \n",
    "        # update args for logging\n",
    "        common_args.update(**peft_config.__dict__)\n",
    "    else:\n",
    "        trainer = custom_training.CustomTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset[\"train\"],\n",
    "            eval_dataset=dataset[\"test\"],\n",
    "            data_collator=custom_training.CustomDataCollatorSeq2Seq(tokenizer, padding=True),\n",
    "        )\n",
    "        \n",
    "        # trainer = SFTTrainer(\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     args=training_args,\n",
    "#     peft_config=lora_config,\n",
    "#     train_dataset=dataset,\n",
    "#     dataset_text_field=\"quote\",\n",
    "# )\n",
    "\n",
    "    import wandb\n",
    "    wandb.init(project=\"mlp-g066-mamba\", name=args.run_name, config=common_args)\n",
    "        \n",
    "    return trainer, training_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    mode = \"interactive\" if IS_INTERACTIVE else \"non-interactive\"\n",
    "\n",
    "    print(\n",
    "        f\"Script is running in {mode} mode\"\n",
    "    )\n",
    "\n",
    "    if IS_INTERACTIVE:\n",
    "        from tqdm.notebook import tqdm\n",
    "\n",
    "        os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"train_local_test\"\n",
    "        from importlib import reload\n",
    "\n",
    "        reload(custom_training)\n",
    "        reload(utils)\n",
    "\n",
    "    # Parse the model name and determine if it should be fetched from a remote source\n",
    "    model_name = utils.parse_model_name(args.base_model, args.from_remote)\n",
    "\n",
    "    if \"mamba\" in args.base_model:\n",
    "        from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "\n",
    "        model = MambaLMHeadModel.from_pretrained(\n",
    "            args.model, dtype=torch.bfloat16, device=\"cuda\"\n",
    "        )\n",
    "    else:\n",
    "        from transformers import AutoModelForCausalLM\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            # load_in_8bit=True,\n",
    "            # device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "    # Print model architecture for the first process in distributed training\n",
    "    if args.local_rank == 0:\n",
    "        print(model)\n",
    "\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    tokenizer = get_tokenizer(model_name)\n",
    "    dataset = get_dataset(args, tokenizer)\n",
    "\n",
    "    # Create a timestamp for model saving\n",
    "    current_time = datetime.now()\n",
    "    formatted_time = current_time.strftime(\"%Y_%m_%d_%H%M\")\n",
    "\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "    tokenizer = get_tokenizer(model_name)\n",
    "\n",
    "    device = (\n",
    "        torch.device(\"cuda\")\n",
    "        if torch.cuda.is_available()\n",
    "        else torch.device(\"mps\")\n",
    "        if torch.backends.mps.is_available()\n",
    "        else torch.device(\"cpu\")\n",
    "    )\n",
    "\n",
    "    model = model.to(device)\n",
    "    trainer, training_args = get_trainer(\n",
    "        args, model, tokenizer, dataset, formatted_time\n",
    "    )\n",
    "\n",
    "    # Clear CUDA cache and start training\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    model.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    global IS_INTERACTIVE\n",
    "    IS_INTERACTIVE = is_interactive()\n",
    "    # Argument parser for command line arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--config\",\n",
    "        default=\"\",\n",
    "        type=str,\n",
    "        help=\"Optional path to JSON configuration file\",\n",
    "    )\n",
    "    parser.add_argument(\"--local_rank\", default=0, type=int)\n",
    "    parser.add_argument(\"--run_name\", default=\"local-test\", type=str)\n",
    "    parser.add_argument(\"--dataset\", required=False, default=\"convfinqa\", type=str)\n",
    "    parser.add_argument(\"--test_dataset\", type=str)\n",
    "    parser.add_argument(\n",
    "        \"--base_model\",\n",
    "        required=False,\n",
    "        default=\"pythia-small\",\n",
    "        type=str,\n",
    "        choices=[\"mamba-small\", \"pythia-small\"],\n",
    "    )\n",
    "    parser.add_argument(\"--max_length\", default=512, type=int)\n",
    "    # parser.add_argument(\"--max_length\", default=2048, type=int)\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\", default=4, type=int, help=\"The train batch size per device\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\", default=1e-4, type=float, help=\"The learning rate\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_epochs\", default=8, type=float, help=\"The training epochs\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gradient_steps\",\n",
    "        default=8,\n",
    "        type=float,\n",
    "        help=\"The gradient accumulation steps\",\n",
    "    )\n",
    "    parser.add_argument(\"--num_workers\", default=8, type=int, help=\"dataloader workers\")\n",
    "    parser.add_argument(\"--log_interval\", default=20, type=int)\n",
    "    parser.add_argument(\"--warmup_ratio\", default=0.05, type=float)\n",
    "    parser.add_argument(\"--ds_config\", default=\"./config_new.json\", type=str)\n",
    "    parser.add_argument(\"--scheduler\", default=\"linear\", type=str)\n",
    "    parser.add_argument(\"--instruct_template\", default=\"default\")\n",
    "    parser.add_argument(\"--evaluation_strategy\", default=\"steps\", type=str)\n",
    "    parser.add_argument(\"--load_best_model\", default=\"False\", type=bool)\n",
    "    parser.add_argument(\"--eval_steps\", default=0.1, type=float)\n",
    "    parser.add_argument(\n",
    "        \"--from_remote\",\n",
    "        default=True,\n",
    "        type=bool,\n",
    "        help=\"Fetch model/data from remote source\",\n",
    "    )\n",
    "    parser.add_argument(\"--prompt_loss_weight\", default=0, type=float)\n",
    "    parser.add_argument(\"--distributed\", default=False, type=bool)\n",
    "    parser.add_argument(\"--fp16\", default=False, type=bool)\n",
    "\n",
    "    global args  # Namespace args\n",
    "    # Parse the command line arguments and update defaults with JSON configuration\n",
    "    config_args, remaining_argv = (\n",
    "        parser.parse_known_args(\"\") if is_interactive() else parser.parse_known_args()\n",
    "    )\n",
    "    json_config_path = (\n",
    "        config_args.config\n",
    "    )  # Use the --config command line argument to specify JSON config file\n",
    "\n",
    "    config_defaults = load_config(json_config_path) if json_config_path else {}\n",
    "\n",
    "    if \"_comment\" in config_defaults:\n",
    "        config_defaults.pop(\"_comment\")\n",
    "    # Update parser defaults based on JSON configuration\n",
    "    parser.set_defaults(**config_defaults)\n",
    "\n",
    "    # Now parse the rest of the arguments with the updated defaults\n",
    "    args = parser.parse_args(remaining_argv)\n",
    "\n",
    "    # Run main\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp_g066_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
