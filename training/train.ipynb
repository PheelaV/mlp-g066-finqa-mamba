{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DISCLAIMER: a lot of this code is take/modified from FinGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "\n",
    "import torch \n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    "    )\n",
    "import datasets\n",
    "\n",
    "import utils\n",
    "import custom_training\n",
    "\n",
    "def is_interactive():\n",
    "    import __main__ as main\n",
    "    return not hasattr(main, \"__file__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(args, model_name):\n",
    "    \"\"\"\n",
    "        Load the tokenizer and set the special tokens, specific to the model\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    if \"mamba\" in args.base_model or \"pythia\" in args.base_model:\n",
    "        # these were defaults either way\n",
    "        tokenizer.eos_token = \"<|endoftext|>\"\n",
    "        tokenizer.pad_token = \"<|padding|>\"\n",
    "        tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)\n",
    "        tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "    return tokenizer\n",
    "\n",
    "def get_dataset(args, tokenizer):\n",
    "    \"\"\"\n",
    "        Load the dataset and apply tokenization\n",
    "    \"\"\"\n",
    "    dataset_list = utils.load_dataset(args.dataset, args.from_remote)\n",
    "    dataset_train = datasets.concatenate_datasets([d['train'] for d in dataset_list]).shuffle(seed=42)\n",
    "    if args.test_dataset:\n",
    "        dataset_list = utils.load_dataset(args.test_dataset, args.from_remote)\n",
    "    dataset_test = datasets.concatenate_datasets([d['test'] for d in dataset_list])\n",
    "    dataset = datasets.DatasetDict({'train': dataset_train, 'test': dataset_test})\n",
    "    # Display first sample from the training dataset\n",
    "    print(dataset['train'][0])\n",
    "    # Filter out samples that exceed the maximum token length and remove unused columns\n",
    "    dataset = dataset.map(partial(utils.tokenize, args, tokenizer, prompt_in_label=True))\n",
    "    print('original dataset length: ', len(dataset['train']))\n",
    "    dataset = dataset.filter(lambda x: not x['exceed_max_length'])\n",
    "    print('filtered dataset length: ', len(dataset['train']))\n",
    "    dataset = dataset.remove_columns(['instruction', 'input', 'output', 'exceed_max_length'])\n",
    "    return dataset\n",
    "\n",
    "def get_trainer(args, model, tokenizer, dataset, formatted_time):\n",
    "    \"\"\"\n",
    "        Create the trainer and training arguments\n",
    "    \"\"\"\n",
    "    if args.distributed:\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"finetuned_models/{args.run_name}_{formatted_time}\",\n",
    "            logging_steps=args.log_interval,\n",
    "            num_train_epochs=args.num_epochs,\n",
    "            per_device_train_batch_size=args.batch_size,\n",
    "            per_device_eval_batch_size=args.batch_size,\n",
    "            gradient_accumulation_steps=args.gradient_steps,\n",
    "            dataloader_num_workers=args.num_workers,\n",
    "            learning_rate=args.learning_rate,\n",
    "            warmup_ratio=args.warmup_ratio,\n",
    "            lr_scheduler_type=args.scheduler,\n",
    "            save_steps=args.eval_steps,\n",
    "            eval_steps=args.eval_steps,\n",
    "            fp16=True,\n",
    "            # fp16_full_eval=True,\n",
    "            deepspeed=args.ds_config,\n",
    "            evaluation_strategy=args.evaluation_strategy,\n",
    "            load_best_model_at_end=args.load_best_model,\n",
    "            remove_unused_columns=False,\n",
    "            report_to=\"wandb\",\n",
    "            run_name=args.run_name,\n",
    "        )\n",
    "        trainer = custom_training.CustomSFTTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset[\"train\"],\n",
    "            eval_dataset=dataset[\"test\"],\n",
    "            data_collator=custom_training.CustomDataCollatorSeq2Seq(tokenizer, padding=True),\n",
    "        )\n",
    "    else:\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"finetuned_models/{args.run_name}_{formatted_time}\",\n",
    "            logging_steps=args.log_interval,\n",
    "            num_train_epochs=args.num_epochs,\n",
    "            # per_device_train_batch_size=args.batch_size,\n",
    "            # per_device_eval_batch_size=args.batch_size,\n",
    "            # gradient_accumulation_steps=args.gradient_steps,\n",
    "            dataloader_num_workers=args.num_workers,\n",
    "            learning_rate=args.learning_rate,\n",
    "            warmup_ratio=args.warmup_ratio,\n",
    "            lr_scheduler_type=args.scheduler,\n",
    "            save_steps=args.eval_steps,\n",
    "            eval_steps=args.eval_steps,\n",
    "            # fp16=True,\n",
    "            # fp16_full_eval=True,\n",
    "            # deepspeed=args.ds_config,\n",
    "            evaluation_strategy=args.evaluation_strategy,\n",
    "            load_best_model_at_end=args.load_best_model,\n",
    "            remove_unused_columns=False,\n",
    "            report_to='wandb',\n",
    "            run_name=args.run_name,\n",
    "        )\n",
    "        trainer = custom_training.CustomTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset[\"train\"],\n",
    "            eval_dataset=dataset[\"test\"],\n",
    "            data_collator=custom_training.CustomDataCollatorSeq2Seq(tokenizer, padding=True),\n",
    "        )\n",
    "    return trainer, training_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    print(f\"Script is running in {'interactive' if IS_INTERACTIVE else 'non-interactive'} mode\")\n",
    "\n",
    "    if IS_INTERACTIVE:\n",
    "        from tqdm.notebook import tqdm\n",
    "        os.environ['WANDB_NOTEBOOK_NAME'] = \"train_local_test\"\n",
    "        from importlib import reload\n",
    "        reload(custom_training)\n",
    "        reload(utils)\n",
    "\n",
    "\n",
    "    # Parse the model name and determine if it should be fetched from a remote source\n",
    "    model_name = utils.parse_model_name(args.base_model, args.from_remote)\n",
    "\n",
    "    if \"mamba\" in args.base_model:\n",
    "        from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "\n",
    "        model = MambaLMHeadModel.from_pretrained(args.model, dtype=torch.bfloat16, device=\"cuda\")\n",
    "    else:\n",
    "        from transformers import AutoModelForCausalLM\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            # load_in_8bit=True,\n",
    "            # device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "    # Print model architecture for the first process in distributed training\n",
    "    if args.local_rank == 0:\n",
    "        print(model)\n",
    "\n",
    "    os.environ['TOKENIZERS_PARALLELISM'] = \"false\"\n",
    "    tokenizer = get_tokenizer(model_name)\n",
    "    dataset = get_dataset(args, tokenizer)\n",
    "\n",
    "    # Create a timestamp for model saving\n",
    "    current_time = datetime.now()\n",
    "    formatted_time = current_time.strftime(\"%Y_%m_%d_%H%M\")\n",
    "\n",
    "    os.environ['TOKENIZERS_PARALLELISM'] = \"true\"\n",
    "    tokenizer = get_tokenizer(model_name)\n",
    "\n",
    "    device = (\n",
    "        torch.device(\"cuda\")\n",
    "        if torch.cuda.is_available()\n",
    "        else torch.device(\"mps\")\n",
    "        if torch.backends.mps.is_available()\n",
    "        else torch.device(\"cpu\")\n",
    "    )\n",
    "\n",
    "    model = model.to(device)\n",
    "    trainer, training_args = get_trainer(args, model, tokenizer, dataset, formatted_time)\n",
    "\n",
    "    # Clear CUDA cache and start training\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    model.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    global IS_INTERACTIVE \n",
    "    IS_INTERACTIVE= is_interactive()\n",
    "    # Argument parser for command line arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--local_rank\", default=0, type=int)\n",
    "    parser.add_argument(\"--run_name\", default=\"local-test\", type=str)\n",
    "    parser.add_argument(\n",
    "        \"--dataset\", required=False, default=\"convfinqa\", type=str\n",
    "    )\n",
    "    parser.add_argument(\"--test_dataset\", type=str)\n",
    "    parser.add_argument(\n",
    "        \"--base_model\",\n",
    "        required=False,\n",
    "        default=\"pythia-small\",\n",
    "        type=str,\n",
    "        choices=[\"mamba-small\", \"pythia-small\"],\n",
    "    )\n",
    "    parser.add_argument(\"--max_length\", default=512, type=int)\n",
    "    # parser.add_argument(\"--max_length\", default=2048, type=int)\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\", default=4, type=int, help=\"The train batch size per device\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\", default=1e-4, type=float, help=\"The learning rate\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_epochs\", default=8, type=float, help=\"The training epochs\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gradient_steps\",\n",
    "        default=8,\n",
    "        type=float,\n",
    "        help=\"The gradient accumulation steps\",\n",
    "    )\n",
    "    parser.add_argument(\"--num_workers\", default=8, type=int, help=\"dataloader workers\")\n",
    "    parser.add_argument(\"--log_interval\", default=20, type=int)\n",
    "    parser.add_argument(\"--warmup_ratio\", default=0.05, type=float)\n",
    "    parser.add_argument(\"--ds_config\", default=\"./config_new.json\", type=str)\n",
    "    parser.add_argument(\"--scheduler\", default=\"linear\", type=str)\n",
    "    parser.add_argument(\"--instruct_template\", default=\"default\")\n",
    "    parser.add_argument(\"--evaluation_strategy\", default=\"steps\", type=str)\n",
    "    parser.add_argument(\"--load_best_model\", default=\"False\", type=bool)\n",
    "    parser.add_argument(\"--eval_steps\", default=0.1, type=float)\n",
    "    parser.add_argument(\"--from_remote\", default=True, type=bool)\n",
    "    parser.add_argument(\"--prompt_loss_weight\", default=0, type=float)\n",
    "    parser.add_argument(\"--distributed\", default=False, type=bool)\n",
    "\n",
    "    # parser.add_argument(\"--from_remote\", default=not IS_INTERACTIVE, type=bool)\n",
    "    global args\n",
    "    args = parser.parse_args(\"\") if is_interactive() else parser.parse_args()\n",
    "\n",
    "    import wandb\n",
    "    config = vars(args)\n",
    "    config.pop(\"ds_config\")\n",
    "    wandb.init(project=\"mlp-g066-mamba\", name=args.run_name, config=config)\n",
    "    # Run main\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# from trl import SFTTrainer\n",
    "# from peft import LoraConfig\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-2.8b-hf\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"state-spaces/mamba-2.8b-hf\")\n",
    "# dataset = load_dataset(\"Abirate/english_quotes\", split=\"train\")\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     num_train_epochs=3,\n",
    "#     per_device_train_batch_size=4,\n",
    "#     logging_dir='./logs',\n",
    "#     logging_steps=10,\n",
    "#     learning_rate=2e-3\n",
    "# )\n",
    "# lora_config =  LoraConfig(\n",
    "#         r=8,\n",
    "#         target_modules=[\"x_proj\", \"embeddings\", \"in_proj\", \"out_proj\"],\n",
    "#         task_type=\"CAUSAL_LM\",\n",
    "#         bias=\"none\"\n",
    "# )\n",
    "# trainer = SFTTrainer(\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     args=training_args,\n",
    "#     peft_config=lora_config,\n",
    "#     train_dataset=dataset,\n",
    "#     dataset_text_field=\"quote\",\n",
    "# )\n",
    "# trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp_g066_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
