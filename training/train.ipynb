{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LLMs\n",
    "optionally LoRA or distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this notebook produces `train.py`, just execute `jupyter nbconvert --to script 'train.ipynb'`\n",
    "\n",
    "## Configuration\n",
    "- configuration will be taken from the defaults\n",
    "- defaults can be overwritten by a supplied json config\n",
    "- everythin can be overwritten by cli arguments\n",
    "\n",
    "\n",
    "**defaults < config < cli**\n",
    "\n",
    "huggingface will cache datasets and their transformations\n",
    "sometimes this might come in handy:\n",
    "`dataset.cleanup_cache_files()` \n",
    "\n",
    "This script will first check if the tokenized dataset is present in `data/` and use that, otherwise it will interface with huggingface and attempt to create a new one. This check is done by `dataset_id` which consists of the name of the particular tokenizer and the maximum sequence length (as the ones over length are ignored)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, TrainingArguments\n",
    "import datasets\n",
    "\n",
    "import utils\n",
    "import custom_training\n",
    "\n",
    "\n",
    "def is_interactive():\n",
    "    import __main__ as main\n",
    "\n",
    "    return not hasattr(main, \"__file__\")\n",
    "\n",
    "\n",
    "def load_config(json_filepath):\n",
    "    if os.path.isfile(json_filepath):\n",
    "        try:\n",
    "            with open(json_filepath, \"r\") as f:\n",
    "                return json.load(f)\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Error loading configuration file {json_filepath}: {e}. Using default command line arguments.\"\n",
    "            )\n",
    "    else:\n",
    "        print(\n",
    "            f\"Configuration file {json_filepath} not found. Using default command line arguments.\"\n",
    "        )\n",
    "    return {}\n",
    "\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\")\n",
    "    \n",
    "if not os.path.exists(\"finetuned_models\"):\n",
    "    os.makedirs(\"finetuned_models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(args, model_name):\n",
    "    \"\"\"\n",
    "    Load the tokenizer and set the special tokens, specific to the model\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    if \"mamba\" in args.base_model or \"pythia\" in args.base_model:\n",
    "        # these were defaults either way\n",
    "        tokenizer.eos_token = \"<|endoftext|>\"\n",
    "        tokenizer.pad_token = \"<|padding|>\"\n",
    "        tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)\n",
    "        tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def get_dataset(args, tokenizer):\n",
    "    \"\"\"\n",
    "    Load the dataset and apply tokenization\n",
    "    \"\"\"\n",
    "\n",
    "    tok_cls_name = (\n",
    "        tokenizer.__class__.__name__[:-4]\n",
    "        if tokenizer.__class__.__name__[-4:] == \"Fast\"\n",
    "        else tokenizer.__class__.__name__\n",
    "    )\n",
    "\n",
    "    # for persistence\n",
    "    dataset_id = f\"{args.dataset}_{args.max_length}_{tok_cls_name}\"\n",
    "    # if dataset is already tokenized, load it\n",
    "    # unless we specifically want the remote version\n",
    "    if (not args.from_remote_data) and os.path.exists(f\"data/{dataset_id}\"):\n",
    "        print(\"Using cached dataset\")\n",
    "        return datasets.load_from_disk(f\"data/{dataset_id}\")\n",
    "    else:\n",
    "        print(\"Loading dataset from remote\")\n",
    "\n",
    "    dataset_list = utils.load_dataset(args.dataset, args.from_remote_data)\n",
    "    dataset_train = datasets.concatenate_datasets(\n",
    "        [d[\"train\"] for d in dataset_list]\n",
    "    ).shuffle(seed=42)\n",
    "    if args.test_dataset:\n",
    "        dataset_list = utils.load_dataset(args.test_dataset, args.from_remote_data)\n",
    "    dataset_test = datasets.concatenate_datasets([d[\"test\"] for d in dataset_list])\n",
    "    dataset = datasets.DatasetDict({\"train\": dataset_train, \"test\": dataset_test})\n",
    "    # Display first sample from the training dataset\n",
    "    # print(dataset[\"train\"][0])\n",
    "    # Filter out samples that exceed the maximum token length and remove unused columns\n",
    "    dataset = dataset.map(\n",
    "        partial(utils.tokenize, args, tokenizer, prompt_in_label=True)\n",
    "    )\n",
    "    print(\"original dataset length: \", len(dataset[\"train\"]))\n",
    "    dataset = dataset.filter(lambda x: not x[\"exceed_max_length\"])\n",
    "    print(\"filtered dataset length: \", len(dataset[\"train\"]))\n",
    "    dataset = dataset.remove_columns(\n",
    "        [\"instruction\", \"input\", \"output\", \"exceed_max_length\"]\n",
    "    )\n",
    "\n",
    "    dataset.save_to_disk(f\"data/{dataset_id}\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_trainer(args, model, tokenizer, dataset, formatted_time):\n",
    "    \"\"\"\n",
    "    Create the trainer and training arguments\n",
    "    \"\"\"\n",
    "\n",
    "    common_args = {\n",
    "        \"output_dir\": f\"finetuned_models/{args.run_name}_{formatted_time}\",\n",
    "        \"logging_steps\": args.log_interval,\n",
    "        \"num_train_epochs\": args.num_epochs,\n",
    "        \"dataloader_num_workers\": args.num_workers,\n",
    "        \"learning_rate\": args.learning_rate,\n",
    "        \"warmup_ratio\": args.warmup_ratio,\n",
    "        \"lr_scheduler_type\": args.scheduler,\n",
    "        \"save_steps\": args.eval_steps,\n",
    "        \"eval_steps\": args.eval_steps,\n",
    "        \"evaluation_strategy\": args.evaluation_strategy,\n",
    "        \"load_best_model_at_end\": args.load_best_model,\n",
    "        \"remove_unused_columns\": False,\n",
    "        \"report_to\": \"wandb\",\n",
    "        \"run_name\": args.run_name,\n",
    "        \"fp16\": args.fp16 & torch.cuda.is_available(),\n",
    "        \"logging_dir\": \"./logs\",\n",
    "        \"label_names\":[]\n",
    "    }\n",
    "\n",
    "    if args.distributed:\n",
    "        distributed_args = {\n",
    "            \"deepspeed\": args.ds_config,\n",
    "            \"per_device_train_batch_size\": args.batch_size,\n",
    "            \"per_device_eval_batch_size\": args.batch_size,\n",
    "            \"gradient_accumulation_steps\": args.gradient_steps,\n",
    "        }\n",
    "        common_args.update(distributed_args)\n",
    "\n",
    "    training_args = TrainingArguments(**common_args)\n",
    "\n",
    "    if args.lora_r > 0:\n",
    "        from peft import (\n",
    "            LoraConfig,\n",
    "            # get_peft_model,\n",
    "            TaskType,\n",
    "        )\n",
    "\n",
    "        if \"mamba\" in args.base_model:\n",
    "            peft_config = LoraConfig(\n",
    "                r=args.lora_r,\n",
    "                target_modules=utils.lora_module_dict[args.base_model],\n",
    "                task_type=TaskType.CAUSAL_LM,\n",
    "                bias=\"none\",\n",
    "            )\n",
    "        else:\n",
    "            peft_config = LoraConfig(\n",
    "                task_type=TaskType.CAUSAL_LM,\n",
    "                inference_mode=False,\n",
    "                r=args.lora_r,\n",
    "                lora_alpha=32,\n",
    "                lora_dropout=0.1,\n",
    "                target_modules=utils.lora_module_dict[args.base_model],\n",
    "                bias=\"none\",\n",
    "            )\n",
    "        trainer = custom_training.CustomSFTTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            peft_config=peft_config,\n",
    "            train_dataset=dataset[\"train\"],\n",
    "            eval_dataset=dataset[\"test\"],\n",
    "            data_collator=custom_training.CustomDataCollatorSeq2Seq(\n",
    "                tokenizer, padding=True\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # update args for logging\n",
    "        common_args.update(**peft_config.__dict__)\n",
    "    else:\n",
    "        trainer = custom_training.CustomTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset[\"train\"],\n",
    "            eval_dataset=dataset[\"test\"],\n",
    "            data_collator=custom_training.CustomDataCollatorSeq2Seq(\n",
    "                tokenizer, padding=True\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # trainer = SFTTrainer(\n",
    "    #     model=model,\n",
    "    #     tokenizer=tokenizer,\n",
    "    #     args=training_args,\n",
    "    #     peft_config=lora_config,\n",
    "    #     train_dataset=dataset,\n",
    "    #     dataset_text_field=\"quote\",\n",
    "    # )\n",
    "\n",
    "    import wandb\n",
    "\n",
    "    wandb.init(project=\"mlp-g066-mamba\", name=args.run_name, config=common_args)\n",
    "\n",
    "    return trainer, training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    mode = \"interactive\" if IS_INTERACTIVE else \"non-interactive\"\n",
    "\n",
    "    if args.local_rank == 0:\n",
    "        print(\n",
    "            f\"Script is running in {mode} mode\"\n",
    "        )\n",
    "\n",
    "    if IS_INTERACTIVE:\n",
    "        from tqdm.notebook import tqdm\n",
    "\n",
    "        os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"train.ipynb\"\n",
    "        from importlib import reload\n",
    "\n",
    "        reload(custom_training)\n",
    "        reload(utils)\n",
    "\n",
    "    # Parse the model name and determine if it should be fetched from a remote source\n",
    "    model_name = utils.parse_model_name(args.base_model, args.from_remote_model)\n",
    "    if args.local_rank == 0:\n",
    "        print(f\"Using model: {model_name}\")\n",
    "    \n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "    tokenizer = get_tokenizer(args, model_name)\n",
    "    dataset = get_dataset(args, tokenizer)\n",
    "    if args.local_rank == 0:\n",
    "        print(f\"Dataset loaded: {dataset}\")\n",
    "\n",
    "    if \"mamba\" in args.base_model:\n",
    "        from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "\n",
    "        model = MambaLMHeadModel.from_pretrained(\n",
    "            model_name, dtype=torch.bfloat16, device=\"cuda\"\n",
    "        )\n",
    "    else:\n",
    "        from transformers import AutoModelForCausalLM\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            # load_in_8bit=True,\n",
    "            # device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "    # Print model architecture for the first process in distributed training\n",
    "    if args.local_rank == 0:\n",
    "        print(model)\n",
    "        \n",
    "    # Create a timestamp for model saving\n",
    "    current_time = datetime.now()\n",
    "    formatted_time = current_time.strftime(\"%Y_%m_%d_%H%M\")\n",
    "\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "    tokenizer = get_tokenizer(args, model_name)\n",
    "\n",
    "    device = (\n",
    "        torch.device(\"cuda\")\n",
    "        if torch.cuda.is_available()\n",
    "        else torch.device(\"mps\")\n",
    "        if torch.backends.mps.is_available()\n",
    "        else torch.device(\"cpu\")\n",
    "    )\n",
    "\n",
    "    if args.local_rank == 0:\n",
    "        print(f\"Commencing training on device: {device}, time: {formatted_time}\")\n",
    "    model = model.to(device)\n",
    "    trainer, training_args = get_trainer(\n",
    "        args, model, tokenizer, dataset, formatted_time\n",
    "    )\n",
    "\n",
    "    # Clear CUDA cache and start training\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # return\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    model.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script is running in interactive mode\n",
      "Using model: EleutherAI/pythia-70m-deduped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached dataset\n",
      "Dataset loaded: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels', 'prompt_lens'],\n",
      "        num_rows: 61\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels', 'prompt_lens'],\n",
      "        num_rows: 3\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 512)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXAttention(\n",
      "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
      ")\n",
      "Commencing training on device: mps, time: 2024_03_09_1500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168b2d0c57134c409ecfcde09cc225ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "GPTNeoXForCausalLM.forward() got an unexpected keyword argument 'prompt_lens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 87\u001b[0m\n\u001b[1;32m     84\u001b[0m args \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_args(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m is_interactive() \u001b[38;5;28;01melse\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mparse_args()\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Run main\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m main(args)\n",
      "Cell \u001b[0;32mIn[19], line 76\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     73\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# return\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Save the fine-tuned model\u001b[39;00m\n\u001b[1;32m     79\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(training_args\u001b[38;5;241m.\u001b[39moutput_dir)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/mlp_g066_training/lib/python3.11/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1540\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1541\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1542\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1543\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1544\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/mlp_g066_training/lib/python3.11/site-packages/transformers/trainer.py:1929\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1926\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 1929\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[1;32m   1930\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1931\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/mlp_g066_training/lib/python3.11/site-packages/transformers/trainer.py:2291\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2289\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 2291\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys_for_eval)\n\u001b[1;32m   2292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2294\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/mlp_g066_training/lib/python3.11/site-packages/transformers/trainer.py:3095\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3092\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3094\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3095\u001b[0m output \u001b[38;5;241m=\u001b[39m eval_loop(\n\u001b[1;32m   3096\u001b[0m     eval_dataloader,\n\u001b[1;32m   3097\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3098\u001b[0m     \u001b[38;5;66;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;00m\n\u001b[1;32m   3099\u001b[0m     \u001b[38;5;66;03m# self.args.prediction_loss_only\u001b[39;00m\n\u001b[1;32m   3100\u001b[0m     prediction_loss_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3101\u001b[0m     ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys,\n\u001b[1;32m   3102\u001b[0m     metric_key_prefix\u001b[38;5;241m=\u001b[39mmetric_key_prefix,\n\u001b[1;32m   3103\u001b[0m )\n\u001b[1;32m   3105\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/mlp_g066_training/lib/python3.11/site-packages/transformers/trainer.py:3284\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3281\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m   3283\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 3284\u001b[0m loss, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_step(model, inputs, prediction_loss_only, ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys)\n\u001b[1;32m   3285\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3286\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_inputs_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/mlp_g066_training/lib/python3.11/site-packages/transformers/trainer.py:3511\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   3509\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3510\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3511\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m   3512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m   3513\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ignore_keys)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/mlp_g066_training/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/mlp_g066_training/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: GPTNeoXForCausalLM.forward() got an unexpected keyword argument 'prompt_lens'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    global IS_INTERACTIVE\n",
    "    IS_INTERACTIVE = is_interactive()\n",
    "    # Argument parser for command line arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--config\",\n",
    "        default=\"config.json\",\n",
    "        type=str,\n",
    "        help=\"Optional path to JSON configuration file\",\n",
    "    )\n",
    "    parser.add_argument(\"--local_rank\", default=0, type=int ,help=\"Local rank for distributed training\")\n",
    "    parser.add_argument(\"--lora_r\", default=0, type=int, help=\"Lora rank, 0 for no lora\")\n",
    "    parser.add_argument(\"--run_name\", default=\"local-test\", type=str)\n",
    "    parser.add_argument(\"--dataset\", required=False, default=\"convfinqa\", type=str)\n",
    "    parser.add_argument(\"--test_dataset\", type=str)\n",
    "    parser.add_argument(\n",
    "        \"--base_model\",\n",
    "        required=False,\n",
    "        default=\"pythia-small\",\n",
    "        type=str,\n",
    "        choices=[\"mamba-small\", \"pythia-small\"],\n",
    "    )\n",
    "    parser.add_argument(\"--max_length\", default=512, type=int)\n",
    "    # parser.add_argument(\"--max_length\", default=2048, type=int)\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\", default=4, type=int, help=\"The train batch size per device\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\", default=1e-4, type=float, help=\"The learning rate\"\n",
    "    )\n",
    "    parser.add_argument(\"--num_epochs\", default=8, type=int, help=\"The training epochs\")\n",
    "    parser.add_argument(\n",
    "        \"--gradient_steps\",\n",
    "        default=8,\n",
    "        type=float,\n",
    "        help=\"The gradient accumulation steps\",\n",
    "    )\n",
    "    parser.add_argument(\"--num_workers\", default=8, type=int, help=\"dataloader workers\")\n",
    "    parser.add_argument(\"--log_interval\", default=20, type=int)\n",
    "    parser.add_argument(\"--warmup_ratio\", default=0.05, type=float)\n",
    "    parser.add_argument(\"--ds_config\", default=\"./config_new.json\", type=str)\n",
    "    parser.add_argument(\"--scheduler\", default=\"linear\", type=str)\n",
    "    parser.add_argument(\"--instruct_template\", default=\"default\")\n",
    "    parser.add_argument(\"--evaluation_strategy\", default=\"steps\", type=str)\n",
    "    parser.add_argument(\"--load_best_model\", default=\"False\", type=bool)\n",
    "    parser.add_argument(\"--eval_steps\", default=0.1, type=float)\n",
    "    parser.add_argument(\n",
    "        \"--from_remote_data\",\n",
    "        default=0,\n",
    "        type=bool,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--from_remote_model\",\n",
    "        default=1,\n",
    "        type=bool,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--prompt_loss_weight\", default=0, type=float, help=\"Prompt loss weight\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--distributed\", default=False, type=bool, help=\"Enable deepspeed\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--fp16\", default=False, type=bool, help=\"Enable fp16 precision\"\n",
    "    )\n",
    "\n",
    "    global args  # Namespace args\n",
    "    # Parse the command line arguments and update defaults with JSON configuration\n",
    "    # config_args, remaining_argv = (\n",
    "    #     parser.parse_known_args(\"\") if is_interactive() else parser.parse_known_args()\n",
    "    # )\n",
    "    config_args = parser.parse_args(\"\") if is_interactive() else parser.parse_args()\n",
    "    json_config_path = (\n",
    "        config_args.config\n",
    "    )  # Use the --config command line argument to specify JSON config file\n",
    "    config_defaults = load_config(json_config_path) if json_config_path else {}\n",
    "    if \"_comment\" in config_defaults:\n",
    "        config_defaults.pop(\"_comment\")\n",
    "    # Update parser defaults based on JSON configuration\n",
    "    parser.set_defaults(**config_defaults)\n",
    "    # Now parse the rest of the arguments with the updated defaults\n",
    "    # args = parser.parse_args(remaining_argv)\n",
    "    args = parser.parse_args(\"\") if is_interactive() else parser.parse_args()\n",
    "\n",
    "    # Run main\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp_g066_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
